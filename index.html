<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chunhui Zhang - RL & Multimodal LLM Researcher</title>
  
  <meta name="author" content="Chunhui Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Chunhui Zhang - Ph.D. student at Dartmouth specializing in RL training and multimodal LLMs. Passionate about building fast, scalable training systems.">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="images/dartmouth_logo.jpeg" type="image/x-icon"/>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chunhui Zhang</name>
              </p>
<p>Greetings! I am Chunhui Zhang (pronounced as Ch'un-hui Chang / å¼µæ˜¥æ™–). I am a Ph.D. candidate in Computer Science at <a href="https://home.dartmouth.edu/">Dartmouth ðŸŒ²</a>, where I have the great fortune of working with <a href="https://www.cs.dartmouth.edu/~soroush/">ðŸŒŸ Professor Soroush Vosoughi</a>. From June to September of 2025, I was a research intern at <a href="https://deepmind.google/">Google DeepMind</a>, Mountain View, CA.</p>

<p>I obtained my MSCS degree (research-based) with the <strong>GSAS Fellowship</strong> at <a href="https://www.brandeis.edu/">Brandeis University</a>. My research journey began at <a href="https://english.neu.edu.cn/">Northeastern University</a>, where I obtained a Bachelor's degree in CS and received the <strong>Outstanding Honor Thesis Award</strong>.</p>

              <p style="text-align:center">
                <a href="mailto:chunhui.zhang.gr@dartmouth.edu"><i class="fa fa-envelope" style="font-size:21px"></i></a> &nbsp/&nbsp
                <a href="https://github.com/chunhuizng"><i class="fa fa-github" style="font-size:23px"></i></a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/chunhui-zhang-541827161/"><i class="fa fa-linkedin" style="font-size:22px"></i></a> &nbsp/&nbsp
                <a href="https://twitter.com/chunhuizng"><i class="fa fa-twitter" style="font-size:23px"></i></a> &nbsp/&nbsp
                <a href="data/Chunhui_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com.hk/citations?hl=en&user=jlqnbkAAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:26%;max-width:26%">
              <a href="images/img.png"><img style="width:70%;max-width:70%" alt="profile photo" src="images/img.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
          <hr style="border:0; height:1px; background-image:linear-gradient(to right, transparent, #ddd, transparent);">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">

              <heading>Research</heading>
<p>
 My work focuses on <b>scaling modular cognitive architectures</b> of multimodal models, particularly supporting their <b>memory capacity</b> and <b>long-range behaviors</b>. I specialize in optimizing training pipelines and accelerating RL systems for complex multimodal and agentic LLM training.
</p>
            </td>
          </tr>
        </tbody></table>
<hr style="border:0; height:1px; background-image:linear-gradient(to right, transparent, #ddd, transparent);">



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">

                <heading>News</heading>
<p>
  <span style="font-style: italic;">[Aug 21, 2025]</span> Three papers accepted to EMNLP 2025. Thanks to my excellent collaborators. See you in Suzhou!
<br>
<p>
  <span style="font-style: italic;">[Jun 2025]</span> Started as a research intern at <a href="https://deepmind.google/">Google DeepMind, Mountain View, CA</a>. Excited to meet friends in the Bay Area!
<br>
<p>
  <span style="font-style: italic;">[May 15, 2025]</span> Four papers accepted at ACL 2025 (one selected as oral presentation). Thanks to my excellent collaborators. See you in Vienna!
<br>	
<p>
  <span style="font-style: italic;">[May 01, 2025]</span> Spotlight at ICML 2025 (Top 2.59%). Thanks to my excellent collaborators. Happy to revisit Vancouver!
<br>	
<p>
  <span style="font-style: italic;">[Apr 23, 2025]</span> Released <a href="https://github.com/chunhuizng/audio-long-form-reasoner" target="_blank">audio-long-form-reasoner</a> on GitHub. First to implement Qwen2.5-Omni and vLLM for faster RL reasoning across audio and other unified modalities.
<br>
<p>
  <span style="font-style: italic;">[Jan 22, 2025]</span> Three papers accepted to NAACL 2025. Thanks to my excellent collaborators. See you in Albuquerque!
<br>	

            </td>
          </tr>
        </tbody></table>

<hr style="border:0; height:1px; background-image:linear-gradient(to right, transparent, #ddd, transparent);">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">

              <heading>Selected Publications</heading>
              <p style="font-size:95%;">
                <b>â˜… Core Contributions</b> â€” Key papers about my work on RL training and multimodal LLM reasoning. <a href="https://scholar.google.com.hk/citations?hl=en&user=jlqnbkAAAAAJ&view_op=list_works&sortby=pubdate">Full publication list (30+ papers)</a>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fdnas_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                </video></div>
                <img src='images/tom.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/abs/2506.01301">
                <papertitle>â˜… Overcoming Multi-step Complexity in Theory-of-Mind Reasoning: A Scalable Bayesian Planner</papertitle>
              </a>
              <br>
              <b>Chunhui Zhang</b>, Zhongyu Ouyang, Kwonjoon Lee, Nakul Agarwal, Sean Dae Houlihan, Soroush Vosoughi, Shao-Yuan Lo
              <br>
                <em>ICML 2025</em> â€“ <strong>Spotlight (Top 2.59%)</strong>
              <br>
              <a href="https://github.com/chunhuizng/scale-bayesian-planner">Code</a> / <a href="https://arxiv.org/pdf/2506.01301">Paper</a>
              <br>
              <p style="font-size:90%; margin-top:8px; color:#555;">
                First scalable solution for multi-step Theory-of-Mind reasoning. Uses Bayesian inverse planning for global planning, then lets LLMs focus on local reasoning. Works on 70B+ models where others fail.
              </p>
            </td>
          </tr>

	  <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fdnas_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                </video></div>
                <img src='images/ACL25_Episodic_Memory.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/abs/2506.01312">
                <papertitle>â˜… Growing Through Experience: Scaling Episodic Grounding in Language Models</papertitle>
              </a>
              <br>
              <b>Chunhui Zhang</b>, Sirui Wang, Zhongyu Ouyang, Xiangchi Yuan, Soroush Vosoughi
              <br>
                <em>ACL 2025</em> â€“ <strong>Oral Presentation (Top 3.24%)</strong>
              <br>
              <a href="https://github.com/chunhuizng/episodic-mcts/tree/main">Code</a> / <a href="https://arxiv.org/pdf/2506.01312">Paper</a>
              <br>
              <p style="font-size:90%; margin-top:8px; color:#555;">
                Post-trained reasoning LLMs (across 3B, 8B, 70B) on MCTS-sampled data from physical simulators.
              </p>
            </td>
          </tr>

          <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fdnas_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                </video></div>
                <img src='images/video-captioner.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/pdf/2502.13363">
                <papertitle>â˜… Pretrained Image-Text Models are Secretly Video Captioners</papertitle>
              </a>
              <br>
              <b>Chunhui Zhang*</b>, Yiren Jian*, Zhongyu Ouyang, Soroush Vosoughi
	      <br>
	      <em>NAACL 2025 Main Conference</em> â€“ <strong>Oral Presentation (Top 2.88%)</strong>
	      <br>
              <a href="https://github.com/chunhuizng/mllm-video-captioner">Code</a> / <a href="https://arxiv.org/pdf/2502.13363">Paper</a> / <a href="data/naacl-video-captioner-slides.pdf">Slides</a>
              <br>
              <p style="font-size:90%; margin-top:8px; color:#555;">
                RL post-training recipe that achieved <b>Top-2</b> on PapersWithCode video captioning leaderboard, outperforming industry MLLM video captioners.
              </p>
            </td>
          </tr>
		  
	<tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fdnas_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                </video></div>
                <img src='images/vla-emnlp2025.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="">
                <papertitle>â˜… Knowing More, Acting Better: Hierarchical Representation for Embodied Decision-Making</papertitle>
              </a>
              <br>
              <b>Chunhui Zhang</b>, Zhongyu Ouyang, Xingjian Diao, Zheyuan Liu, Soroush Vosoughi
              <br>
                <em>EMNLP 2025 Findings</em>
		<br>
              <a href="data/EMNLP25_Embodied_Hierarchy.pdf">Paper</a>
              <br>
              <p style="font-size:90%; margin-top:8px; color:#555;">
                Refines vision-language-action LLM representations to enable more effective PPO-based RL training in embodied AI.
              </p>
            </td>
          </tr>

	<tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fdnas_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                </video></div>
                <img src='images/working-memory.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="">
                <papertitle>Working Memory Identifies Reasoning Limits in Language Models</papertitle>
              </a>
              <br>
              <b>Chunhui Zhang</b>, Yiren Jian, Zhongyu Ouyang, Soroush Vosoughi
              <br>
                <em>EMNLP 2024</em>
              <br>
              <a href="https://github.com/chunhuizng/working-memory-limits">Code</a> / <a href="data/EMNLP24_Working_Memory.pdf">Paper</a>
              <br>
              <p style="font-size:90%; margin-top:8px; color:#555;">
                Introduced working memory as a diagnostic tool for LLM reasoning limits. This work inspired a follow-up NAACL paper on long-context multimodal understanding.
              </p>
            </td>
          </tr>

    <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/twm.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/pdf/2502.06020">
                <papertitle>Temporal Working Memory: Query-Guided Segment Refinement for Enhanced Multimodal Understanding</papertitle>
              </a>
              <br>
              Xingjian Diao*, <b>Chunhui Zhang*</b>, Weiyi Wu, Zhongyu Ouyang, Peijun Qing, Ming Cheng, Soroush Vosoughi, Jiang Gui
              <br>
                <em>NAACL 2025 Findings</em>
		<br>
              <a href="https://github.com/xid32/NAACL_2025_TWM">Code</a> / <a href="https://xid32.github.io/images/publications/Temporal_Working_Memory.pdf">Paper</a>
              <br>
              <p style="font-size:90%; margin-top:8px; color:#555;">
                Inspired by working memory in my EMNLP 2024 paper, this work is a follow-up study on long-context video-language understanding.
              </p>
            </td>
    </tr>
        </tbody></table>



<hr style="border:0; height:1px; background-image:linear-gradient(to right, transparent, #ddd, transparent);">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Collaborations</heading>
              <p style="font-size:90%;">
                Projects where I contributed as co-author. <a href="https://scholar.google.com.hk/citations?hl=en&user=jlqnbkAAAAAJ&view_op=list_works&sortby=pubdate">View all publications â†’</a>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/emnlp2025-soundmind.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/pdf/2506.12935">
                <papertitle>SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models</papertitle>
              </a>
              <br>
              Xingjian Diao, <b>Chunhui Zhang</b>, Keyi Kong, Weiyi Wu, Chiyu Ma, Zhongyu Ouyang, Peijun Qing, Soroush Vosoughi, Jiang Gui
              <br>
                <em>EMNLP 2025</em> â€“ <strong>Oral Presentation</strong>
              <br>
              <a href="https://arxiv.org/pdf/2506.12935">Paper</a>
            </td>
          </tr>

	<tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/reasoning.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/pdf/2503.02103">
                <papertitle>Superficial Self-Improved Reasoners Benefit from Model Merging</papertitle>
              </a>
              <br>
              Xiangchi Yuan, <b>Chunhui Zhang</b>, Zheyuan Liu, Dachuan Shi, Soroush Vosoughi, Wenke Lee
              <br>
                <em>EMNLP 2025</em>
              <br>
              <a href="https://github.com/xiangchi-yuan/merge_syn">Code</a> / <a href="https://arxiv.org/pdf/2503.02103">Paper</a>
            </td>
          </tr>

        <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/vlm.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://arxiv.org/abs/2310.03291">
                <papertitle>Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction</papertitle>
              </a>
              <br>
              Yiren Jian, Tingkai Liu, Yunzhe Tao, <b>Chunhui Zhang</b>, Soroush Vosoughi, Hongxia Yang
              <br>
                <em>ACL 2024</em> â€“ <strong>Oral Presentation (Top 3.10%)</strong>
              <br>
              <a href="https://github.com/yiren-jian/EVLGen">Code</a> / <a href="https://arxiv.org/abs/2310.03291">Paper</a>
            </td>
          </tr>

        <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/datadec.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://openreview.net/forum?id=3jV525Hmqr">
                <papertitle>When Sparsity Meets Contrastive Models: Less Data Can Bring Better Class-Balanced Representations</papertitle>
              </a>
              <br>
              <b>Chunhui Zhang</b>, Chao Huang, Yijun Tian, Qianlong Wen, Zhongyu Ouyang, Youhuan Li, Yanfang Ye, et al.
              <br>
        <em>ICML 2023</em> â€“ <strong>AAAI-DCAA 2023 Best Paper Runner-up Award</strong>
              <br>
              <a href="https://openreview.net/forum?id=3jV525Hmqr">Paper</a>
            </td>
          </tr>

        <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/iclr_game.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://openreview.net/pdf?id=7jk5gWjC18M">
                <papertitle>Chasing All-Round Graph Representation Robustness: Model, Training, and Optimization</papertitle>
              </a>
              <br>
              <b>Chunhui Zhang</b>, Yijun Tian, Mingxuan Ju, Zheyuan Liu, Yanfang Ye, Nitesh Chawla, et al.
              <br>
        <em>ICLR 2023</em>
              <br>
              <a href="https://github.com/chunhuizng/GAME">Code</a> / <a href="https://openreview.net/pdf?id=7jk5gWjC18M">Paper</a>
            </td>
          </tr>

        <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/lip_align.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://github.com/chunhuizng/lipschitz-fairness">
                <papertitle>Aligning Relational Learning with Lipschitz Fairness</papertitle>
              </a>
              <br>
              Yaning Jia*, <b>Chunhui Zhang*</b>, Soroush Vosoughi
              <br>
                <em>ICLR 2024</em> (<i>*co-first author</i>)
              <br>
              <a href="https://github.com/chunhuizng/lipschitz-fairness">Code</a> / <a href="data/ICLR24_Lipschitz_Align_camera_ready.pdf">Paper</a>
            </td>
          </tr>

        <tr onmouseout="CMI_stop()" onmouseover="sgcl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/emergent-degradation.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://openreview.net/forum?id=Koh0i2u8qX">
                <papertitle>Mitigating Emergent Robustness Degradation on Graphs while Scaling Up</papertitle>
              </a>
              <br>
              Xiangchi Yuan*, <b>Chunhui Zhang*</b>, Yijun Tian, Yanfang Ye, et al.
              <br>
                <em>ICLR 2024</em> (<i>*co-first author</i>)
              <br>
              <a href="https://github.com/chunhuizng/emergent-degradation">Code</a> / <a href="https://openreview.net/forum?id=Koh0i2u8qX">Paper</a>
            </td>
          </tr>

        </tbody></table>

<hr style="border:0; height:1px; background-image:linear-gradient(to right, transparent, #ddd, transparent);">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>To All Students/Friends</heading>
              <p>
Feel free to drop me an email if you're up for a laid-back chat about life, career, or research. I'm dedicating (at least) 30 minutes each week for these discussions, and I'm particularly eager to connect with students from underrepresented backgrounds or those dealing with challenges or inequity. Just reach outâ€”I'm here to listen!
              </p>
            </td>
          </tr>
        </tbody></table>

<hr style="border:0; height:1px; background-image:linear-gradient(to right, transparent, #ddd, transparent);">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Fun Stuff</heading>
              <p>
                Racingâ€”a happy part of my life. I particularly enjoy go-karting and circuit racing (some fun facts:
                  <a href="https://www.instagram.com/p/Ce-IBtFsBUl/">1st</a> and <a href="https://www.instagram.com/p/CgV3251sPcJ/">2nd</a> place at Supercharged). But there is one type of racing that I have yet to tryâ€”my favorite rally driving (My favorite rally driver is Han Han).
              </p>
            </td>
          </tr>
        </tbody></table>

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=IMj5QOauaOh3itXiv54K4YbC6WTpio8QQs1f5DGrnzo&cl=ffffff&w=120&h=80"></script>
<div style="height:1px; margin-top:20px; margin-bottom:20px; background-image:linear-gradient(to right, transparent, #ddd, transparent);"></div>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">this cool guy</a>.
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
